# predict<-NULL
# nobs<-NULL
# for(i in 1:nrow(additional.wordlist)){
#   curr.term<-additional.wordlist[i,1]
#   term.index<-which(names(chi.aw.predict)==curr.term)
#   curr.predict<-chi.aw.predict[term.index]
#   curr.obs<-chi.nobs[term.index]
#   predict<-c(predict, curr.predict)
#   nobs<-c(nobs, curr.obs)
# }


#the following code iterates a logistic regression over a sequece of variables (randomizing the combination) and then stores
#the resulting effect sizes per variable in a list
listAdd<-function(add.feature, feature.name, composite.list){
  match.index<-which(names(composite.list)==feature.name)
  composite.list[[match.index]]<-c(composite.list[[match.index]], add.feature)
  return(composite.list)
}

#num.feat is the number of features per iteration of the model
#group vars is the numerical variables that indicate group membership (0 and 1)
#num.iter is the number of iterations of the function
#REMBER TO SCALE THE FULL FEATURE TABLE FIRST!
modelIterate<-function(full.feature.table, num.feat, num.iter, group.vars){
  ptm<-proc.time()
  composite.list<-vector(mode="list", length=ncol(full.feature.table))
  names(composite.list)<-colnames(full.feature.table)
  for(i in 1:num.iter){
    var.select<-sample(colnames(full.feature.table), num.feat)
    cut.table<-full.feature.table[which(colnames(full.feature.table) %in% var.select)]
    cut.table$Group<-group.vars
    model.fit<-glm(Group~., data=cut.table, family=binomial(link="logit"))
    var.effects<-coef(summary(model.fit))[,4]
    var.effects<-var.effects[-1]
    for(j in 1:length(var.effects)){
      composite.list<-listAdd(var.effects[j], names(var.effects[j]), composite.list)
    }
  }
  print(proc.time()-ptm)
  return(composite.list)
}

#code to iterate the above across multiple cores
parModelIterate<-function(full.feature.table, num.feat, num.iter, group.vars){
  library(parallel)
  ncores<-detectCores()-2
  seq.list<-seq(1, ncores, by=1)
  clust.proc<-makeCluster(ncores, type="FORK")
  all.results<-parLapply(clust.proc, seq.list, function(x) modelIterate(full.feature.table, num.feat, num.iter, group.vars))
  stopCluster(clust.proc)
  #keys<-unique(unlist(lapply(all.results, names)))
  #all.results<-setNames(do.call(mapply, c(FUN=c, lapply(all.results, '[', keys))), keys)
  return(all.results)
}

#code to collapse list of lists of vectors generated by parallel code above
#assumes that all lists are the same length and in the same order
listCombine<-function(full.lol){
  composite.list<-full.lol[[1]]
  for(i in 2:length(full.lol)){
    current.lol<-full.lol[[i]]
    composite.list<-mapply(function(x,y) c(unlist(x),unlist(y)), composite.list, current.lol, SIMPLIFY = F)
  }
  return(composite.list)
}

#functions to return summary stats from the combined lol
featureStats<-function(effect.vector, freq.table, count.table, group.vars, feature.name){
  freq.vector<-freq.table[,which(colnames(freq.table)==feature.name)]
  count.vector<-count.table[,which(colnames(count.table)==feature.name)]
  mean.effect<-mean(effect.vector)
  total.models<-length(effect.vector)
  perc.sig<-(length(which(effect.vector<0.05)))/total.models
  perc.high.sig<-(length(which(effect.vector<0.000001)))/total.models
  mean.freq.human<-mean(freq.vector[which(group.vars==1)])
  mean.freq.obj<-mean(freq.vector[which(group.vars==0)])
  total.count.human<-sum(count.vector[which(group.vars==1)])
  total.count.object<-sum(count.vector[which(group.vars==0)])
  min.effect<-min(effect.vector)
  max.effect<-max(effect.vector)
  stat.vector<-c(mean.effect, total.models, perc.sig, perc.high.sig, max.effect, min.effect, mean.freq.human, mean.freq.obj, total.count.human, total.count.object)
  return(stat.vector)
}


allFeatureStats<-function(composite.list, freq.feature.table, count.feature.table, group.vars){
  feature.names<-names(composite.list)
  all.stat.features<-mapply(function(x,y) featureStats(x, freq.feature.table, count.feature.table, group.vars, y), composite.list, feature.names, SIMPLIFY = F)
  all.stat.features<-do.call("rbind", all.stat.features)
  colnames(all.stat.features)<-c("Mean_Effect_Size", "Total_Models_with_Feature", "PercEffects<0.05", "PercEffect<0.000001", "Max_Effect_Size", "Min_Effect_Size", "Mean_Freq_with_Human", "Mean_Freq_with_Object", "Total_Count_with_Human", "Total_Count_with_Objects")
  rownames(all.stat.features)<-names(composite.list)
  return(all.stat.features)
}

#make model
#takes a wordlist with categories (trimmed for the two categories in the model)
#also takes a test proportion to withold (i.e. .33 for 33%) - if this is null - it only makes the model and returns that
#creates a logistic regression model - returns the model and if a test set is used, the class table for the test set and the correctness percentage
#uses "group.names" to assign 1 to the first element and 0 to the second
#finally returns a copy of the full scaled feature table
#if feature.set=Null,then usues all features in the table. Otherwise, subselects only those features
#wordcheck is for the next set of code below - if T, it will return only the list of words misclassified, as well as the perc.correct
makeModel<-function(wordlist, feature.table, feature.set=NULL, test.proportion=.33, group.names=c("Human", "Object"), word.check=F){
  feature.table<-feature.table[which(rownames(feature.table) %in% wordlist[,1]),]
  feature.table<-feature.table[,which(colnames(feature.table) %in% feature.set)]
  wordlist<-wordlist[which(wordlist[,1] %in% rownames(feature.table)),]
  feature.table<-feature.table[order(rownames(feature.table)),]
  wordlist<-wordlist[order(wordlist[,1]),]
  feature.groups<-wordlist[,2]
  groups.num<-rep(0, length(feature.groups))
  groups.num[which(feature.groups==group.names[1])]<-1
  row.sums<-rowSums(feature.table)
  bad.rows<-which(row.sums==0)
  if(length(bad.rows)>0){
    feature.table<-feature.table[-bad.rows,]
    feature.groups<-feature.groups[-bad.rows]
    groups.num<-groups.num[-bad.rows]
  }
  feature.scaled<-feature.table/rowSums(feature.table)
  feature.scaled<-as.data.frame(feature.scaled, stringsAsFactors=F)
  if(!is.null(test.proportion)){
    num.test<-floor(test.proportion*nrow(feature.scaled))
    test.index<-sample(nrow(feature.scaled), num.test)
    feature.test<-feature.scaled[test.index,]
    feature.train<-feature.scaled[-test.index,]
    groups.train<-groups.num[-test.index]
    groups.test<-feature.groups[test.index]
    feature.train$Group<-groups.train
    model.fit<-glm(Group~., data=feature.train, family=binomial(link="logit"))
    test.results<-predict(model.fit, newdata=feature.test, type="response")
    results.term<-ifelse(test.results > 0.05, group.names[1], group.names[2])
    class.table<-table(results.term, groups.test)
    perc.correct<-100-(((length(which(!results.term==groups.test)))/length(results.term))*100)
    if(!word.check){
      final.list<-list(model.fit, feature.scaled, groups.num, class.table, perc.correct)
      names(final.list)<-c("Model", "Feature_Table", "Groups", "Test_Classification_Table", "PercentCorrect")
      return(final.list)
    } else {
      misclassed.terms<-rownames(feature.test[which(!results.term==groups.test),])
      terms.tested<-rownames(feature.test)
      final.list<-list(misclassed.terms, perc.correct, terms.tested)
      names(final.list)<-c("Misclassed_Terms", "Perc_Correct", "Terms_Tested")
      return(final.list)
    }
  } else {
    feature.train<-feature.scaled
    feature.train$Group<-groups.num
    model.fit<-glm(Group~., data=feature.train, family=binomial(link="logit"))
    final.list<-list(model.fit, feature.scaled, groups.num)
    names(final.list)<-c("Model", "Feature_Table", "Groups")
    return(final.list)
  }
}


#code to see which words are being consistently misclassified by the model (if any)
misclassTerms<-function(wordlist, feature.table, feature.set, n.iter){
  ptm<-proc.time()
  iter.set<-seq(1, n.iter, by=1)
  all.results<-lapply(iter.set, function(x) makeModel(wordlist, feature.table, feature.set, word.check=T))
  perc.correct<-lapply(all.results, function(x) x$Perc_Correct)
  #bad.models<-which(is.na(perc.correct))
  #all.results<-all.results[-bad.models]
  #perc.correct<-perc.correct[-bad.models]
  misclassed.terms<-lapply(all.results, function(x) x$Misclassed_Terms)
  terms.tested<-unlist(lapply(all.results, function(x) x$Terms_Tested))
  terms.tested<-table(terms.tested)
  terms.tested<-terms.tested[order(names(terms.tested))]
  misclassed.terms<-table(unlist(misclassed.terms))
  misclassed.terms<-misclassed.terms[order(names(misclassed.terms))]
  terms.tested<-terms.tested[which(names(terms.tested) %in% names(misclassed.terms))]
  unique.terms<-names(misclassed.terms)
  names(misclassed.terms)<-NULL
  names(terms.tested)<-NULL
  freq.misclassed<-misclassed.terms/terms.tested
  feature.list<-list(misclassed.terms, terms.tested, freq.misclassed)
  misclassed.table<-do.call("cbind", feature.list)
  misclassed.table<-data.frame(unique.terms, misclassed.table, stringsAsFactors=F)
  #misclassed.table<-data.frame(unique.terms, misclassed.terms, terms.tested, freq.misclassed, stringsAsFactors = F)
  colnames(misclassed.table)<-c("Term", "NumberOfTimesMisclassed", "NumberOfTimesTested", "FreqMisclassed")
  misclassed.table<-misclassed.table[order(misclassed.table[,4], decreasing=T),]
  #misclassed.terms<-sort(misclassed.terms, decreasing=T)
  perc.correct<-mean(unlist(perc.correct))
  final.list<-list(misclassed.table, perc.correct)
  names(final.list)<-c("Misclassed_Term_Table", "MeanPercCorrect")
  print(proc.time()-ptm)
  return(final.list)
}






